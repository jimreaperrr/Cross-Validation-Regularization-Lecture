{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bias Variance, Cross Validation, & Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11/26/18\n",
    "\n",
    "### Created by Roland Chin, Nichole Sun, Michelle Hao\n",
    "##### with material from Ajay Raj, Nichole Sun, Rosa Choe and Data 100 Lecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from plotting import overfittingDemo, plot_multiple_linear_regression, ridgeRegularizationDemo, lassoRegularizationDemo\n",
    "from scipy.optimize import curve_fit\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "sns.set()\n",
    "sns.set_context('talk')\n",
    "np.set_printoptions(threshold=20, precision=2, suppress=True)\n",
    "pd.options.display.max_rows = 7\n",
    "pd.options.display.max_columns = 8\n",
    "pd.set_option('precision', 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='recap'></a>\n",
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](fit_graphs.png \"Fit Graphs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bias corresponds to underfitting. If we look at the first model, the points seem to follow some sort of curve, but our predictor is linear and therefore, unable to capture all the points. In this case, we have chosen a model which is not complex enough to accurately capture all the information from our data set. \n",
    "\n",
    "If we look at the last model, the predictor is now overly complex because it adjusts based on every point in order to get as close to every data point as possible. In this case, the model changes too much based on small fluctuations caused by insignificant details in the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='bv-tradeoff'></a>\n",
    "## Bias-Variance Tradeoff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Today we'll perform **model evaluation**, where we'll judge how our linear regression models actually perform. We will require a **loss function**, which describes a numerical value for how far your model is from the true values.\n",
    "\n",
    "$$\\textit{RSS} = \\sum_{i=0}^n {e_i}^2 = \\sum_{i=0}^n (y_i - mx_i - b)^2$$\n",
    "\n",
    "Now we will generalize this formula: Say that there are $p$ features, or independent variables. Your task is to create a model $\\hat{f}$, such that the loss is now:\n",
    "\n",
    "$$\\frac{1}{n} \\sum_i^n (y_i - \\hat{f}(x_i))^2$$\n",
    "\n",
    "In this loss function, $y_i$ is a number, and $x_i$ is a $p$-vector, because there are $p$ features. This loss is called **mean squared error**, or **MSE**.\n",
    "\n",
    "Now, we'll talk about other ways to evaluate a model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's define some terms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can say that everything in the universe can be described with the following equation:\n",
    "\n",
    "$$y = h(x) + \\epsilon$$\n",
    "\n",
    "- $y$ is the quantity you are trying to model\n",
    "- $x$ are the parameters (independent variables)\n",
    "- $h$ is the **true model** for $y$ in terms of $x$\n",
    "- $\\epsilon$ represents **noise**, a random number which has mean zero\n",
    "\n",
    "Let $\\hat{f}$ be your model for $y$ in terms of $x$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "### Bias\n",
    "\n",
    "When evaluating a model, the most intuitive first step is to look at how well the model performs. For classification, this may be the percentage of data points correctly classified, or for regression it may be how close the predicted values are to actual. The **bias** of a model is a measure of how close our prediction is to the actual value on average from an average model. Note that bias is not a measure of a single model, it encapsulates the scenario in which we collect many datasets, create models for each dataset, and average the error over all of models. Bias is not a measure of error for a single model, but a more abstract concept describing the average error over all errors. A low value for the bias of a model describes that on average, our predictions are similar to the actual values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variance\n",
    "The **variance** of  a model relates to the variance of the distribution of all models. In the previous section about bias, we envisioned the scenario of collecting many datasets, creating models for each dataset, and averaging the error overall the datasets. Instead, the variance of a model describes the variance in prediction. While we might be able to predict a value very well on average, if the variance of predictions is very high this may not be very helpful, as when we train a model we only have one such instance, and a high model variance tells us little about the true nature of the predictions. A low variance describes that our model will not predict very different values for different datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](BiasVariance.jpg \"Bias Variance Visualization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The image describes what bias and variance are in a more simplified example. Consider that we would like to create a model that selects a point close to the center. The models on the top row have low bias, meaning the center of the cluster is close to the red dot on the target. The models on the left column have low variance, the clusters are quite tight, meaning our predictions are close together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Tradeoff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are trying to minimize **expected error**, or the average **MSE** over all datasets. It turns out (with some advanced probability gymnastics), that:\n",
    "\n",
    "$$\\text{Expected Error} = \\text{Noise Variance} + \\text{Bias}^2 + \\text{Variance}$$\n",
    "\n",
    "Note that $\\text{Noise Variance}$ is constant: we assume there is some noise, and $\\text{Noise Variance}$ is simply a value that describes how noisy your dataset will be on average."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This equation defines what is known as the **bias variance tradeoff**. \n",
    "\n",
    "![alt text](BiasVarianceTradeoff.png \"Bias Variance Tradeoff\")\n",
    "\n",
    "Image from http://scott.fortmann-roe.com/docs/BiasVariance.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why is this true intuitively?\n",
    "\n",
    "At some point as we decrease **bias**, instead of getting closer to the **true model** $h$, we go past and try to fit to the $\\epsilon$ (noise) that is part of our current dataset. This is equivalent to making our model more noisy: which means that over all datasets, it has more **variance**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions for understanding**:\n",
    "> 1. Where does underfitting and overfitting lie in the graph above? How do they relate to bias and variance?\n",
    "> 2. Why can't we usually just make a bunch of models with low bias and high variance and average them?\n",
    "> 3. Why is low variance important in models?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polynomial Regression\n",
    "\n",
    "Let's look at a polynomial problem\n",
    "\n",
    "In this case, if our model has degree $d$, we have $d + 1$ features: $x = [x^0, x^1, ..., x^d]$. Now, we have a linear model with $d + 1$ features:\n",
    "\n",
    "$$\\hat{f}(x) = \\sum_{i=0}^{d} a_i x_i$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model complexity in this case is the degree of the polynomial. As we saw last week, as $d$ increases, model complexity increases. The model gets better, but then gets erratic. This directly corresponds to the bias-variance graph above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "overfittingDemo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at these models, we can tell the best model is a degree 3 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mpg = pd.read_csv(\"mpg.csv\", index_col='name')# load mpg dataset\n",
    "mpg = mpg.loc[mpg[\"horsepower\"] != '?'].astype(float) # remove columns with missing horsepower values\n",
    "mpg_train, mpg_test = train_test_split(mpg, test_size = .2, random_state = 0) # split into training set and test set\n",
    "mpg_train, mpg_validation = train_test_split(mpg_train, test_size = .5, random_state = 0)\n",
    "mpg_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X1rLzRsW37jc"
   },
   "source": [
    "## Cross-Validation\n",
    "\n",
    "Our goal is to find some kind of happy medium between underfitting and overfitting, so our model fits well against **the underlying distribution of our data** and also **generalizes well against other data points**.\n",
    "\n",
    "We always want a more accurate way of seeing what the test error is in order to manage the bias-variance trade off. As we have seen when we create a model, training error is misleadingly low due to overfitting since we are fitting our model on the training set. When we predict on the test set afterwards, this error might be very high since we haven't seen anything other than the training data.\n",
    "\n",
    "A solution to this is to use cross validation, in which we split our original data into training, test, and validation data. This lets us repeatedly estimate our model error by testing our model multiple times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why? Because Bias-Variance Tradeoff\n",
    "\n",
    "Cross-validation helps us manage the bias-variance tradeoff more accurately.\n",
    "\n",
    "The validation error estimates test error by checking the model's performance on a dataset that isn't the training data, which allows us to estimate model bias and model variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-Validation-Test Split\n",
    "\n",
    "The original dataset is split into 3 subsets:\n",
    "\n",
    "* Training set: models are fit on this data (~70%)\n",
    "* Validation set: we select the best features/hyperparameters (things which you set that cannot be learned) from here (~15%)\n",
    "* Test set: the final set used for determining the model's accuracy (~15%)\n",
    "\n",
    "We want to have a good balance between the 3 data sets; a larger training set will decrease the model accuracy but this means the validation and test sets may be too small and not representative of the original data.\n",
    "\n",
    "Then we select a model and a set of features by doing the following:\n",
    "\n",
    "1. For every potential set of features, fit a model using the training set. The error of a model on the training set is its *training error*.\n",
    "1. Check the error of each model on the validation set, which is its *validation error*. Select the model that achieves the lowest validation error. This will be our choice of features, hyperparameters, and model.\n",
    "1. Calculate the *test error*, which is just the error of the best model we've chosen on the last data set, the test set. From here, we cannot adjust the features or model to decrease test error, since this changes the test set into a validation set. What we can do is find a new test set.\n",
    "\n",
    "This process allows us to more accurately determine the model to use than using the training error alone. By using cross-validation, we can test our model on data that it wasn't fit on, simulating test error without using the test set. This gives us a sense of how our model performs on unseen data.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mean Squared Error:** The error output from finding the average of all the squared magnitudes from all the predicted and actual outputs. This is the type of error \"function\" we use to calculate training and test errors. To get a better idea of what **MSE** is, take a look at the following picture:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='mean_squared_error.png' width=400, height=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Error and Test Error\n",
    "\n",
    "A model is \"bad\" if it fails to gather anything from unseen data (test set). The test error is how we determine how accurate our model performs since we've never seen it before.\n",
    "\n",
    "The training error decreases as we make our model more complex with additional features and such, but having a really low training error doesn't always mean our model is perfect, since our model may just be overfitting the more complex it is.\n",
    "\n",
    "We can see how the test and training error change as complexity changes below.\n",
    "\n",
    "![feature_train_test_error.png](https://raw.githubusercontent.com/DS-100/textbook/master/assets/feature_train_test_error.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "48opVFox3_VW"
   },
   "source": [
    "## Even Better: K-Fold Cross-Validation\n",
    "\n",
    "We can improve on the **train-validation-test split** method. With train-validation-test, making 3 splits results in too little data for training.\n",
    "\n",
    "Thus we can run the train-validation split multiple times on the same dataset. The training dataset is divided into *k* equally-sized subsets, and the train-validation split is repeated *k* times. Every time, one of the *k* subsets, or folds, is used as the validation set, and the remaining *k - 1* folds are used for its training. \n",
    "\n",
    "The model's validation error is the average of it's $k$ validation errors. \n",
    "\n",
    "Here is an example with 5 folds.\n",
    "\n",
    "![feature_5_fold_cv.jpg](https://github.com/DS-100/textbook/blob/master/assets/feature_5_fold_cv.jpg?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NPV2Jr8_A1ES"
   },
   "source": [
    "A benefit of this is that every data point is used for validation exactly once and for training *k-1* times.\n",
    "\n",
    "If *k* is small, the error estimate has lower variance (many validation points) but higher bias (fewer training points).\n",
    "\n",
    "If *k* is large, the error estimate has lower bias but higher variance. \n",
    "\n",
    "A disadvantage of this model is that it takes more computation time, but it computes a more accurate validation error n the end.\n",
    "\n",
    "The `scikit-learn` library provides a convenient [`sklearn.model_selection.KFold`](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html) class to implement $k$-fold cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Model Selection for Ice Cream Ratings\n",
    "Here we have a simple example in which we utilize cross-validation to select a model in order to predict ice cream ratings from ice cream sweetness.\n",
    "\n",
    "First we visualize the data below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignore this cell\n",
    "ice = pd.read_csv('icecream.csv')\n",
    "transformer = PolynomialFeatures(degree=2)\n",
    "X = transformer.fit_transform(ice[['sweetness']])\n",
    "\n",
    "clf = LinearRegression(fit_intercept=False).fit(X, ice[['overall']])\n",
    "xs = np.linspace(3.5, 12.5, 300).reshape(-1, 1)\n",
    "rating_pred = clf.predict(transformer.transform(xs))\n",
    "\n",
    "temp = pd.DataFrame(xs, columns = ['sweetness'])\n",
    "temp['overall'] = rating_pred\n",
    "\n",
    "np.random.seed(42)\n",
    "x_devs = np.random.normal(scale=0.2, size=len(temp))\n",
    "y_devs = np.random.normal(scale=0.2, size=len(temp))\n",
    "temp['sweetness'] = np.round(temp['sweetness'] + x_devs, decimals=2)\n",
    "temp['overall'] = np.round(temp['overall'] + y_devs, decimals=2)\n",
    "\n",
    "ice = pd.concat([temp, ice])\n",
    "ice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignore this cell\n",
    "plt.scatter(ice['sweetness'], ice['overall'])\n",
    "plt.title('Ice Cream Rating vs. Sweetness')\n",
    "plt.xlabel('Sweetness')\n",
    "plt.ylabel('Rating');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use a degree 10 polynomial on 9 random points to create a perfectly accurate model for these points. But this is an example of overfitting, which means the model fails to generalize to unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ignore this cell\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "ice2 = pd.read_csv('icecream.csv')\n",
    "trans_ten = PolynomialFeatures(degree=10)\n",
    "X_ten = trans_ten.fit_transform(ice2[['sweetness']])\n",
    "y = ice2['overall']\n",
    "\n",
    "clf_ten = LinearRegression(fit_intercept=False).fit(X_ten, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignore this cell\n",
    "np.random.seed(1)\n",
    "x_devs = np.random.normal(scale=0.4, size=len(ice2))\n",
    "y_devs = np.random.normal(scale=0.4, size=len(ice2))\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.scatter(ice2['sweetness'], ice2['overall'])\n",
    "xs = np.linspace(3.5, 12.5, 1000).reshape(-1, 1)\n",
    "ys = clf_ten.predict(trans_ten.transform(xs))\n",
    "plt.plot(xs, ys)\n",
    "plt.title('Degree 10 polynomial fit')\n",
    "plt.ylim(3, 7);\n",
    "\n",
    "plt.subplot(122)\n",
    "ys = clf_ten.predict(trans_ten.transform(xs))\n",
    "plt.plot(xs, ys)\n",
    "plt.scatter(ice2['sweetness'] + x_devs, ice2['overall'] + y_devs, c='g')\n",
    "plt.title('Degree 10 poly, second set of data')\n",
    "plt.ylim(3, 7);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's follow what we learned. First we first partition our data into training, validation, and test datasets using `scikit-learn`'s [`sklearn.model_selection.train_test_split`](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) method to perform a 70/30% train-test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "test_size = 92\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(ice[['sweetness']], ice['overall'], test_size=test_size, random_state=0)\n",
    "\n",
    "print(f'  Training set size: {len(X_train)}')\n",
    "print(f'      Test set size: {len(X_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to fit polynomial regression models using the training set, from polynomial degrees 1-10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# First, we add polynomial features to X_train\n",
    "transformers = [PolynomialFeatures(degree=deg) for deg in range(1, 11)]\n",
    "X_train_polys = [transformer.fit_transform(X_train) for transformer in transformers]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will then perform 5-fold cross-validation on the 10 featurized datasets. To do so, we will create a function that:\n",
    "1. Uses the [`KFold.split`](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html) function to get 5 splits on the training data. (`split` returns the indices of the data for that split)\n",
    "2. For each split, select the rows and columns based on the split indices and features.\n",
    "3. Fit a linear model on the training split.\n",
    "4. Compute the mean squared error on the validation split.\n",
    "5. Return the average error across all cross validation splits.\n",
    "\n",
    "This may sound complicated, but is actually a pretty standard procedure which sklearn allows us to easily do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "def mse_cost(y_pred, y_actual):\n",
    "    return np.mean((y_pred - y_actual) ** 2)\n",
    "\n",
    "def compute_CV_error(model, X_train, Y_train):\n",
    "    kf = KFold(n_splits=5)\n",
    "    validation_errors = []\n",
    "    \n",
    "    for train_idx, valid_idx in kf.split(X_train):\n",
    "        # split the data\n",
    "        split_X_train, split_X_valid = X_train[train_idx], X_train[valid_idx]\n",
    "        split_Y_train, split_Y_valid = Y_train.iloc[train_idx], Y_train.iloc[valid_idx]\n",
    "\n",
    "        # Fit the model on the training split\n",
    "        model.fit(split_X_train, split_Y_train)\n",
    "        \n",
    "        # Compute the RMSE on the validation split\n",
    "        error = mse_cost(split_Y_valid, model.predict(split_X_valid))\n",
    "        \n",
    "        # Add this split's error to a list with all of the errors\n",
    "        validation_errors.append(error)\n",
    "    \n",
    "    #average all the validation errors\n",
    "    return np.mean(validation_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We train a linear regression classifier for each featurized dataset and perform cross-validation\n",
    "cross_validation_errors = [compute_CV_error(LinearRegression(fit_intercept=False), X_train_poly, y_train) for X_train_poly in X_train_polys]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that as we use higher degree polynomial features, the validation error decreases and increases again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignore this cell\n",
    "from IPython.core.display import display\n",
    "cv_df = pd.DataFrame({'Validation Error': cross_validation_errors}, index=range(1, 11))\n",
    "cv_df.index.name = 'Degree'\n",
    "pd.options.display.max_rows = 20\n",
    "display(cv_df)\n",
    "pd.options.display.max_rows = 7\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.plot(cv_df.index, cv_df['Validation Error'])\n",
    "plt.scatter(cv_df.index, cv_df['Validation Error'])\n",
    "plt.title('Validation Error vs. Polynomial Degree')\n",
    "plt.xlabel('Polynomial Degree')\n",
    "plt.ylabel('Validation Error');\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.plot(cv_df.index, cv_df['Validation Error'])\n",
    "plt.scatter(cv_df.index, cv_df['Validation Error'])\n",
    "plt.ylim(0.044925, 0.05)\n",
    "plt.title('Zoomed In')\n",
    "plt.xlabel('Polynomial Degree')\n",
    "plt.ylabel('Validation Error')\n",
    "\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After examining the validation errors, we see that the best, most accurate model is that of a degree 2 polynomial features. Thus, we select the degree 2 polynomial model as our final model and fit it on the training data.\n",
    "\n",
    "Finally, we compute its error on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_trans = transformers[1]\n",
    "best_model = LinearRegression(fit_intercept=False).fit(X_train_polys[1], y_train)\n",
    "\n",
    "training_error = mse_cost(best_model.predict(X_train_polys[1]), y_train)\n",
    "validation_error = cross_validation_errors[1]\n",
    "test_error = mse_cost(best_model.predict(best_trans.transform(X_test)), y_test)\n",
    "\n",
    "print('Degree 2 polynomial')\n",
    "print(f'  Training error: {training_error:0.5f}')\n",
    "print(f'Validation error: {validation_error:0.5f}')\n",
    "print(f'      Test error: {test_error:0.5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the test error is higher than the validation error which is higher than the training error.\n",
    "\n",
    "This makes sense because the model is fit on the training data, which minimizes the mean squared error for that dataset. The validation error and the test error are usually always higher than the training error because these errors are computed on unknown datasets.\n",
    "\n",
    "In the future, `scikit-learn` has a [`cross_val_predict`](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_predict.html) method to automatically perform cross-validation, so we don't have to methodically break the data into training and validation sets ourselves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EHnBEAmGnb6M"
   },
   "source": [
    "## Cross Validation Summary\n",
    "\n",
    "The end goal of the cross-validation technique is to manage the bias-variance tradeoff and find an optimal model.\n",
    "\n",
    "**In conclusion**, we can see that validation can be an incredibly useful process through which we create a model that fits, but doesn't overfit or underfit, on our data. We also see that different measurable quantities, such as training error and test error, can also be indicative of how well our model is doing at different periods of time, and see how well it's fitting against our data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='regularization'></a>\n",
    "## Regularization \n",
    "As we add lot of features, this typically increases the variance of the model, resulting in worse performance overall. However, the features may contain important information about our data, so we may not want to throw them out completely. Regularization is a method that allows us to penalize complexity, while still allowing us to incorporate as much information as possible. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that the ordinary least squares model is the following, where $\\hat{\\theta}$ is the model weights, and $x$ is the vector of features:\n",
    "$$f_\\hat{\\theta}(x) = \\hat{\\theta} \\cdot x$$\n",
    "\n",
    "When we find the best fit for our model, we want to minimize the square differences, so we want to find $\\hat{\\theta}$ such that: \n",
    "$$\\hat{\\boldsymbol{\\theta}} = \\arg\\!\\min_\\theta \\sum_{i=0}^n (y_i - f_\\boldsymbol{\\theta}(x_i))^2$$\n",
    "\n",
    "This equation above is the ordinary least squares model __without regularization__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know that having large weights for more features increases variance. Because of this, we want to add regularization in order to discourage our model from overfitting. This is done by adding a $R(\\boldsymbol{\\theta})$ term to penalize large weight values. \n",
    "\n",
    "Here's what our ordinary least squares model looks like __with regularization__:\n",
    "\n",
    "$$\\hat{\\boldsymbol{\\theta}} = \\arg\\!\\min_\\theta \\sum_{i=0}^n (y_i - f_\\boldsymbol{\\theta}(x_i))^2 + \\lambda R(\\boldsymbol{\\theta})$$\n",
    "\n",
    "The new term after the summation is the **regularization** term. The $\\lambda$ parameter in front of it dictates how limiting our regularization term is – the higher $\\lambda$ is, the more we penalize large weights, and the more the regularization makes our weights deviate from OLS. \n",
    "\n",
    "**Question**: What happens when $\\lambda = 0$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, what is $R(\\theta)$? It could be a lot of things! Today we'll talk about two of the most common regularization functions – ridge and LASSO. \n",
    "\n",
    "<table>\n",
    "    <tr><td>Ridge</td><td>L2 Norm</td><td>$R(\\boldsymbol{\\theta}) = \\sum\\limits_{i=0}^n \\theta_i^2$</td></tr>\n",
    "    <tr><td>LASSO</td><td>L1 Norm</td><td>$R(\\boldsymbol{\\theta}) = \\sum\\limits_{i=0}^n \\lvert\\theta_i\\rvert$</td></tr>\n",
    "</table>\n",
    "\n",
    "<a id='ridge'></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='L2 Regression: Ridge Regression'></a>\n",
    "### L2 Regression: Ridge Regression\n",
    "One way we could penalize the weights is penalizing the sum of the squared weights.  \n",
    "For **ridge regression** the penalty is,\n",
    "$$R(\\boldsymbol{\\theta}) = \\sum\\limits_{i=0}^n \\theta_i^2$$\n",
    "\n",
    "Substituting this regularization function into the least squared model from before, our regularized model now looks as follows:\n",
    "$$\\hat{\\boldsymbol{\\theta}} = \\arg\\!\\min_\\theta \\sum_{i=0}^n (y_i - f_\\boldsymbol{\\theta}(x_i))^2 + \\lambda \\sum\\limits_{i=0}^n \\theta_i^2$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Something interesting about Ridge Regression is that there is always a unique, mathematical solution that can be found using a known formula. The solution involves linear algebra, so you don't need to know it, but the existence of this formula also makes it computationally easy to solve.\n",
    "$$\\hat{\\boldsymbol{\\theta}} = \\left(\\boldsymbol{X}^T \\boldsymbol{X} + \\lambda\\boldsymbol{I}\\right)^{-1}\\boldsymbol{X}^T\\boldsymbol{Y}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to truly see the effect of the regularization, lets first create a __regular linear regression model__ that we can compare to the regularized models. \n",
    "We will use a polynomial model with `displacement` up to degree 20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "x_train = np.vander(mpg_train[\"displacement\"], 13)\n",
    "y_train = mpg_train[[\"mpg\"]]\n",
    "\n",
    "x_validation = np.vander(mpg_validation[\"displacement\"], 13)\n",
    "y_validation = mpg_validation[[\"mpg\"]]\n",
    "\n",
    "# instantiate your model\n",
    "linear_model = ...\n",
    "\n",
    "# fit the model\n",
    "...\n",
    "# make predictions on validation set\n",
    "linear_prediction = ...\n",
    "# find mean squared error\n",
    "linear_loss = ...\n",
    "\n",
    "print(\"Root Mean Squared Error of linear model: {:.2f}\".format(linear_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using what you did above as reference, do the same using a Ridge regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "...\n",
    "ridge_loss = ... # mean squared error of ridge model\n",
    "\n",
    "print(\"Root Mean Squared Error of linear model: {:.2f}\".format(linear_loss))\n",
    "print(\"Root Mean Squared Error of ridge model: {:.2f}\".format(ridge_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='L1 Regularization: LASSO'></a>\n",
    "### L1 Regularization: LASSO\n",
    "In **LASSO**, we penalize the sum of absolute values of the weights. So,\n",
    "$$R(\\boldsymbol{\\theta}) = \\sum\\limits_{i=0}^n \\lvert\\theta_i\\rvert$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If there's one thing you should know about LASSO is that it is *sparsity inducing*. This just means that it forces some weights to take on zero values, leaving you with fewer explanatory variables in the resulting model than you put in. Unlike ridge regression, LASSO doesn't necessarily have a unique solution, and there's no formula that determines what the optimal weights should be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "...\n",
    "lasso_loss = ... # mean squared error of lasso model\n",
    "\n",
    "print(\"Root Mean Squared Error of linear model: {:.2f}\".format(linear_loss))\n",
    "print(\"Root Mean Squared Error of lasso model: {:.2f}\".format(lasso_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Ridge and LASSO\n",
    "We just told you a lot of things about ridge and lasso, but here are some visualizations to help you understand the intuition behind some of the characteristics of these two regularization methods. Another way to describe the modified minimization function above is that it's the same loss function as before, with the *additional constraint* that $R(\\boldsymbol{\\theta}) \\leq t$. Now, $t$ is related to $\\lambda$ but the exact relationship between the two parameters depends on your data. Regardless, let's take a look at what this means in the two-dimensional case. For ridge,\n",
    "\n",
    "$$\\theta_0^2 + \\theta_1^2 \\leq t$$\n",
    "\n",
    "Does this look familiar to you? What if it's in the form $x^2 + y^2 \\leq t$? Or how about now:\n",
    "<img src='http://vikingsseason5i.com/wp-content/uploads/2018/08/circle-equation-circle-equation-unit-circle.jpg' width=400 />\n",
    "\n",
    "Lasso is of the form $$\\left|\\theta_0\\right| + \\left|\\theta_1\\right| \\leq t$$ This one's a little harder to interpret, perhaps this will help inspire you:\n",
    "<img src='https://cdn.kastatic.org/ka-perseus-graphie/3b9b8f4b4dac19e1197e9dd94553d0822f9fe69a.png' />\n",
    "\n",
    "#### Norm Balls\n",
    "<img src='https://upload.wikimedia.org/wikipedia/commons/f/f8/L1_and_L2_balls.svg' width=400/>\n",
    "<img src='norm_balls.png' width=400/>\n",
    "\n",
    "The rhombus and circle as a visualization of the regularization term, while the blue circles are the topological curves representing the loss function based on the weights. You want to minimize the sum of these, which means you want to minimize each of those. The point that minimizes the sum is the minimum point at which they intersect.\n",
    "\n",
    "\n",
    "**Question**: Based on these visualizations, could you explain why LASSO is sparsity-inducing?\n",
    "\n",
    "Turns out that the $L2-norm$ is always some sort of smooth surface, from a circle in 2D to a sphere in 3D. On the other hand, LASSO always has sharp corners. This is exactly the feature that makes it sparsiy-inducing. As you might imagine, just as humans are more likely to bump into sharp corners than smooth surfaces, the loss term is also most likely to intersect the $L2-norm$ at one of the corners.\n",
    "\n",
    "### Regularization and Bias Variance\n",
    "As we mentioned earlier, the bias is the average OLS loss term across multiple models of the same family (e.g. same degree polynomial) trained on separate datasets. Variance is the average variance of the weight vectors (coefficients) on your features. \n",
    "\n",
    "Without the regularization term, we’re just minimizing bias; the regularization term means we won’t get the lowest possible bias, but we’re exchanging that for some lower variance so that our model does better at generalizing to data points outside of our training data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lambda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We said that $\\lambda$ is how much we care about the regularization term, but what does that look like? Let's return to the polynomial example from last week, and see what the resulting models look like with different values of $\\lambda$ given a degree 8 polynomial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ridgeRegularizationDemo([0, 0.5, 1.0, 5.0, 10.0], 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we know what to use for $\\lambda$ (or `alpha` in the `sklearn.linear_model` constructors)?\n",
    "\n",
    "That's right, it's validation! \n",
    "\n",
    "### Validation on Lambda\n",
    "Let's try to find the best $\\lambda$ for the degree 20 polynomial on `displacement` from above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambdas = np.arange(0, 200) # create a list of potential lambda values\n",
    "\n",
    "# create a list containing the corresponding mean_squared_error for each lambda usinb both ridge and lasso regression\n",
    "ridge_errors = [] \n",
    "lasso_errors = []\n",
    "\n",
    "# \n",
    "\n",
    "# finds the index of the minimum value in each list\n",
    "ridge_errors.index(min(ridge_errors)), lasso_errors.index(min(lasso_errors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sanity Check\n",
    "\n",
    "1. What happens as $\\lambda$ increases?\n",
    "    1. bias increases, variance increases\n",
    "    2. bias increases, variance decreases\n",
    "    3. bias decreases, variance increases\n",
    "    4. bias decreases, variance decreases\n",
    "2. **True** or **False**? Bias is how much error your model makes.\n",
    "3. What is **sparsity**?\n",
    "4. For each of the following, choose **ridge**, **lasso**, **both**, or **neither**:\n",
    "    1. L1-norm\n",
    "    2. L2-norm\n",
    "    3. Induces sparsity\n",
    "    4. Has analytic (mathematical) solution\n",
    "    5. Increases bias\n",
    "    6. Increases variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision & Recall\n",
    "\n",
    "There are two kinds of errors our model can make:\n",
    "- False positive (FP), for example a good email gets flagged as spam and filtered out of the inbox\n",
    "- False negative (FN), for example a spam email gets mislabeled as good and ends up in the inbox\n",
    "\n",
    "These definitions depend both on the true labels and the predicted labels. False positives and false negatives may be of differing importance. For example, a false positive for cancer is less impactful than a false negative for cancer, since treatment is crucial if someone does have cancer.\n",
    "\n",
    "Going back to our example with normal and spam emails:\n",
    "\n",
    "**Precision** measures the proportion $\\frac{\\text{TP}}{\\text{TP} + \\text{FP}}$ of emails flagged as spam that are actually spam.\n",
    "\n",
    "**Recall** measures the proportion $\\frac{\\text{TP}}{\\text{TP} + \\text{FN}}$ of spam emails that were correctly flagged as spam. \n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/2/26/Precisionrecall.svg/700px-Precisionrecall.svg.png\" width=\"500px\">\n",
    "\n",
    "**A system with high recall but low precision returns many results, but most of its predicted labels are incorrect.**\n",
    "\n",
    "**A system with high precision but low recall returns very few results, but most of its predicted labels are correct.**\n",
    "\n",
    "**An ideal model yields high precision and high recall, which will return many results, with almost all results labeled correctly.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Say we have 5 emails, whose true labels are the following: 1, 0, 0, 1, 1; here 1 is spam, and 0 is normal.\n",
    "\n",
    "Our model outputs the following predicted labels, say, based on the words in the body of each email: 0, 1, 0, 1, 0.\n",
    "\n",
    "In order to calculate the precision, and recall, we need the number of true positives, false positives, and false negatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_label = np.array([1, 0, 0, 1, 1])\n",
    "predicted_label = np.array([0, 1, 0, 1, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use basic array arithmetic to find each of the above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m----------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-67fa71f72629>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrue_positives\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrue_label\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mpredicted_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mfalse_positives\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount_nonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrue_label\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mpredicted_label\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mfalse_negatives\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount_nonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicted_label\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtrue_label\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"true positives:\\t\\t{}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrue_positives\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "true_positives = np.sum(true_label * predicted_label)\n",
    "false_positives = np.count_nonzero((true_label - predicted_label) == -1)\n",
    "false_negatives = np.count_nonzero((predicted_label - true_label) == -1)\n",
    "\n",
    "print(\"true positives:\\t\\t{}\".format(true_positives))\n",
    "print(\"false positives:\\t{}\".format(false_positives))\n",
    "print(\"false negatives:\\t{}\".format(false_negatives))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = true_positives / (true_positives + false_positives)\n",
    "recall = true_positives / (true_positives + false_negatives)\n",
    "\n",
    "print(\"precision:\\t{}\".format(precision))\n",
    "print(\"recall:\\t\\t{}\".format(recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
